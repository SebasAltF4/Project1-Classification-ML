{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c0ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiquetas de actividades:\n",
      "  id           activity\n",
      "  1            WALKING\n",
      "  2   WALKING_UPSTAIRS\n",
      "  3 WALKING_DOWNSTAIRS\n",
      "  4            SITTING\n",
      "  5           STANDING\n",
      "  6             LAYING\n",
      "X_train shape: (7352, 128, 9)\n",
      "y_train shape: (7352,)\n",
      "['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z']\n",
      "['body_acc_x', 'body_acc_y', 'body_acc_z', 'body_gyro_x', 'body_gyro_y', 'body_gyro_z', 'total_acc_x', 'total_acc_y', 'total_acc_z', 'y']\n",
      "X_test shape: (2947, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "#### CARGA DE DATOS\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Configuración de estilo\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "\n",
    "# Cargar dataset HAR\n",
    "train_file = \"../data/train.h5\"\n",
    "test_file = \"../data/test.h5\"\n",
    "labels_file = \"../data/activity_labels.txt\"\n",
    "\n",
    "# Cargar etiquetas de actividades\n",
    "activity_labels = pd.read_csv(labels_file, sep=\" \", header=None, names=[\"id\", \"activity\"])\n",
    "print(\"Etiquetas de actividades:\\n\", activity_labels.to_string(index=False))\n",
    "\n",
    "# Leer archivo H5 de entrenamiento\n",
    "with h5py.File(train_file, \"r\") as f:\n",
    "    channels = [\n",
    "        np.array(f[\"body_acc_x\"]),\n",
    "        np.array(f[\"body_acc_y\"]),\n",
    "        np.array(f[\"body_acc_z\"]),\n",
    "        np.array(f[\"body_gyro_x\"]),\n",
    "        np.array(f[\"body_gyro_y\"]),\n",
    "        np.array(f[\"body_gyro_z\"]),\n",
    "        np.array(f[\"total_acc_x\"]),\n",
    "        np.array(f[\"total_acc_y\"]),\n",
    "        np.array(f[\"total_acc_z\"]),\n",
    "    ]\n",
    "    y_train = np.array(f[\"y\"]).astype(int)-1  # etiquetas\n",
    "\n",
    "# Apilar para formar un único tensor (n_muestras, 128, 9)\n",
    "X_train = np.stack(channels, axis=-1)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# print(np.min(y_train), np.max(y_train))\n",
    "\n",
    "# n, t, c = X_train.shape   # n = n_muestras, t=128, c=9\n",
    "# Xn = X_train.reshape(n, t * c)\n",
    "# print(\"Xn shape:\", Xn.shape)\n",
    "\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "import h5py\n",
    "\n",
    "with h5py.File(test_file, \"r\") as f:\n",
    "    print(list(f.keys()))\n",
    "\n",
    "with h5py.File(train_file, \"r\") as f:\n",
    "    print(list(f.keys()))\n",
    "\n",
    "with h5py.File(test_file, \"r\") as f:\n",
    "    channels_test = [\n",
    "        np.array(f[\"body_acc_x\"]),\n",
    "        np.array(f[\"body_acc_y\"]),\n",
    "        np.array(f[\"body_acc_z\"]),\n",
    "        np.array(f[\"body_gyro_x\"]),\n",
    "        np.array(f[\"body_gyro_y\"]),\n",
    "        np.array(f[\"body_gyro_z\"]),\n",
    "        np.array(f[\"total_acc_x\"]),\n",
    "        np.array(f[\"total_acc_y\"]),\n",
    "        np.array(f[\"total_acc_z\"]),\n",
    "    ]\n",
    "    #y_test = np.array(f[\"y\"]).astype(int)-1  # etiquetas en 0..5\n",
    "\n",
    "# Apilar canales → tensor (n_muestras, 128, 9)\n",
    "X_testORI = np.stack(channels_test, axis=-1)\n",
    "print(\"X_test shape:\", X_testORI.shape)\n",
    "#print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "def train_and_evaluate_decision_tree(X, y,\n",
    "                                     test_size=0.2,\n",
    "                                     random_state=9,\n",
    "                                     max_depth=5,\n",
    "                                     min_samples_split=2,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     max_features='all',   # 'all' | 'sqrt' | int\n",
    "                                     criterion='gini',     # only 'gini' implemented\n",
    "                                     verbose=True):\n",
    "    \"\"\"\n",
    "    Implementación didáctica de Decision Tree (CART, criterio Gini).\n",
    "    X: ndarray (n_samples, n_features) o DataFrame\n",
    "    y: ndarray (n_samples,)\n",
    "    Si test_size is None -> no se hace split (usa X,y como dataset completo para entrenar y devuelve árbol).\n",
    "    Devuelve dict con árbol, splits y métricas sobre test (si existiera).\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    # --- Helpers ---\n",
    "    def gini(y_subset):\n",
    "        if len(y_subset) == 0:\n",
    "            return 0.0\n",
    "        counts = np.array(list(Counter(y_subset).values()), dtype=float)\n",
    "        p = counts / counts.sum()\n",
    "        return 1.0 - np.sum(p**2)\n",
    "\n",
    "    def weighted_impurity(left, right):\n",
    "        nL, nR = len(left), len(right)\n",
    "        n = nL + nR\n",
    "        if n == 0: return 0.0\n",
    "        return (nL / n) * gini(left) + (nR / n) * gini(right)\n",
    "\n",
    "    def candidate_thresholds(col):\n",
    "        vals = np.unique(col)\n",
    "        if vals.shape[0] <= 10:\n",
    "            return (vals[:-1] + vals[1:]) / 2.0\n",
    "        # muchos valores: muestreamos percentiles para velocidad y evitar demasiados thresholds\n",
    "        pct = np.linspace(0, 100, num=11)[1:-1]  # 9 percentiles\n",
    "        qs = np.percentile(col, pct)\n",
    "        return np.unique(qs)\n",
    "\n",
    "    def best_split(X_arr, y_arr):\n",
    "        best = {'feature_idx': None, 'threshold': None, 'imp': float('inf')}\n",
    "        n_features = X_arr.shape[1]\n",
    "\n",
    "        # seleccionar features segun max_features\n",
    "        if max_features == 'all':\n",
    "            feats = range(n_features)\n",
    "        elif max_features == 'sqrt':\n",
    "            k = max(1, int(np.sqrt(n_features)))\n",
    "            feats = rng.choice(range(n_features), size=k, replace=False)\n",
    "        elif isinstance(max_features, int):\n",
    "            k = min(max_features, n_features)\n",
    "            feats = rng.choice(range(n_features), size=k, replace=False)\n",
    "        else:\n",
    "            feats = range(n_features)\n",
    "\n",
    "        for fi in feats:\n",
    "            col = X_arr[:, fi]\n",
    "            thr_candidates = candidate_thresholds(col)\n",
    "            for thr in thr_candidates:\n",
    "                left_mask = col <= thr\n",
    "                right_mask = col > thr\n",
    "                if left_mask.sum() < min_samples_leaf or right_mask.sum() < min_samples_leaf:\n",
    "                    continue\n",
    "                imp = weighted_impurity(y_arr[left_mask], y_arr[right_mask])\n",
    "                if imp < best['imp']:\n",
    "                    best = {'feature_idx': fi, 'threshold': thr, 'imp': imp}\n",
    "        return best\n",
    "\n",
    "    def majority_class(y_subset):\n",
    "        return Counter(y_subset).most_common(1)[0][0]\n",
    "\n",
    "    # Recursiva\n",
    "    def build_tree(X_arr, y_arr, depth=0):\n",
    "        n = len(y_arr)\n",
    "        classes = np.unique(y_arr)\n",
    "        # condiciones de paro\n",
    "        if (n < min_samples_split) or (depth >= max_depth) or (len(classes) == 1):\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "        split = best_split(X_arr, y_arr)\n",
    "        if split['feature_idx'] is None:\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "\n",
    "        fi, thr = split['feature_idx'], split['threshold']\n",
    "        left_mask = X_arr[:, fi] <= thr\n",
    "        right_mask = X_arr[:, fi] > thr\n",
    "\n",
    "        # proteger recursión infinita\n",
    "        if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "\n",
    "        left = build_tree(X_arr[left_mask], y_arr[left_mask], depth + 1)\n",
    "        right = build_tree(X_arr[right_mask], y_arr[right_mask], depth + 1)\n",
    "        return {'type': 'node',\n",
    "                'feature_idx': int(fi),\n",
    "                'threshold': float(thr),\n",
    "                'left': left,\n",
    "                'right': right,\n",
    "                'n': int(n)}\n",
    "\n",
    "    def predict_single(node, x_row):\n",
    "        while node['type'] != 'leaf':\n",
    "            if x_row[node['feature_idx']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return int(node['class'])\n",
    "\n",
    "    def predict(tree, X_arr):\n",
    "        return np.array([predict_single(tree, row) for row in X_arr], dtype=int)\n",
    "\n",
    "    # ---- Preparar datos ----\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = X.columns.tolist()\n",
    "        X_arr = X.values\n",
    "    else:\n",
    "        X_arr = np.array(X)\n",
    "        feature_names = [f'x{i}' for i in range(X_arr.shape[1])]\n",
    "\n",
    "    y_arr = np.array(y).astype(int)\n",
    "\n",
    "    # Validar que y no tenga valores negativos o no enteros\n",
    "    if np.any(y_arr < 0):\n",
    "        raise ValueError(\"y contiene etiquetas negativas, revisar el encoding de clases.\")\n",
    "\n",
    "    # split train/test\n",
    "    if test_size is None:\n",
    "        X_tr, y_tr = X_arr, y_arr\n",
    "        X_te, y_te = None, None\n",
    "    else:\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_arr, y_arr, test_size=test_size,\n",
    "            stratify=y_arr if len(np.unique(y_arr))>1 else None,\n",
    "            random_state=random_state)\n",
    "\n",
    "    # construir árbol\n",
    "    tree = build_tree(X_tr, y_tr)\n",
    "\n",
    "    # predecir sobre test si existe\n",
    "    if X_te is not None:\n",
    "        y_pred_test = predict(tree, X_te)\n",
    "        cm = confusion_matrix(y_te, y_pred_test)\n",
    "        f1 = f1_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "        prec = precision_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "    else:\n",
    "        y_pred_test = None\n",
    "        cm = None; f1 = None; prec = None; rec = None\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Decision Tree (impl. propia) — métricas (test):\")\n",
    "        print(\"max_depth:\", max_depth, \"min_samples_split:\", min_samples_split,\n",
    "              \"min_samples_leaf:\", min_samples_leaf, \"max_features:\", max_features)\n",
    "        if cm is not None:\n",
    "            print(\"Confusion matrix:\\n\", cm)\n",
    "            print(f\"F1 (weighted): {f1:.4f}  Precision: {prec:.4f}  Recall: {rec:.4f}\")\n",
    "        else:\n",
    "            print(\"No test split provided (test_size=None).\")\n",
    "\n",
    "    return {\n",
    "        'tree': tree,\n",
    "        'feature_names': feature_names,\n",
    "        'X_train': X_tr, 'X_test': X_te, 'y_train': y_tr, 'y_test': y_te,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'confusion_matrix': cm,\n",
    "        'f1': f1, 'precision': prec, 'recall': rec\n",
    "    }\n",
    "\n",
    "def predict_from_tree(tree, X_arr):\n",
    "    \"\"\"Predict robusto sobre un árbol construido por la función anterior.\"\"\"\n",
    "    def predict_one(node, x_row):\n",
    "        while node['type'] != 'leaf':\n",
    "            if x_row[node['feature_idx']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return int(node['class'])\n",
    "    return np.array([predict_one(tree, row) for row in X_arr], dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601044d",
   "metadata": {},
   "source": [
    "## Decision Tree Optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8393edf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_time_domain_features(X_sequence):\n",
    "    \"\"\"\n",
    "    Extrae características del dominio temporal para cada canal de la secuencia\n",
    "    X_sequence: (128, 9) - 128 timesteps, 9 canales\n",
    "    Returns: array de características (1D)\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for channel in range(X_sequence.shape[1]):  # Para cada canal (9)\n",
    "        signal = X_sequence[:, channel]\n",
    "        \n",
    "        # Características básicas\n",
    "        mean = np.mean(signal)\n",
    "        std = np.std(signal)\n",
    "        var = np.var(signal)\n",
    "        rms = np.sqrt(np.mean(signal**2))\n",
    "        \n",
    "        # Características de forma\n",
    "        skewness = stats.skew(signal)\n",
    "        kurt = stats.kurtosis(signal)\n",
    "        \n",
    "        # Valores extremos\n",
    "        max_val = np.max(signal)\n",
    "        min_val = np.min(signal)\n",
    "        peak_to_peak = max_val - min_val\n",
    "        \n",
    "        # Energía y potencia\n",
    "        energy = np.sum(signal**2)\n",
    "        \n",
    "        # Características estadísticas adicionales\n",
    "        median = np.median(signal)\n",
    "        mad = np.mean(np.abs(signal - mean))  # mean absolute deviation\n",
    "        \n",
    "        # Percentiles\n",
    "        p25, p50, p75 = np.percentile(signal, [25, 50, 75])\n",
    "        iqr = p75 - p25\n",
    "        \n",
    "        features.extend([mean, std, var, rms, skewness, kurt, \n",
    "                        max_val, min_val, peak_to_peak, energy,\n",
    "                        median, mad, p25, p50, p75, iqr])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def extract_frequency_domain_features(X_sequence):\n",
    "    \"\"\"\n",
    "    Extrae características del dominio frecuencial para cada canal\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for channel in range(X_sequence.shape[1]):\n",
    "        signal = X_sequence[:, channel]\n",
    "        \n",
    "        # Transformada rápida de Fourier\n",
    "        fft_vals = np.fft.fft(signal)\n",
    "        fft_magnitude = np.abs(fft_vals)[:len(fft_vals)//2]  # Solo frecuencias positivas\n",
    "        \n",
    "        if len(fft_magnitude) > 0:\n",
    "            # Características espectrales\n",
    "            spectral_centroid = np.sum(np.arange(len(fft_magnitude)) * fft_magnitude) / np.sum(fft_magnitude)\n",
    "            spectral_energy = np.sum(fft_magnitude**2)\n",
    "            spectral_entropy = -np.sum(fft_magnitude * np.log2(fft_magnitude + 1e-12))\n",
    "            \n",
    "            # Frecuencias dominantes\n",
    "            dominant_freq1 = np.argmax(fft_magnitude)\n",
    "            max_magnitude = np.max(fft_magnitude)\n",
    "            \n",
    "            features.extend([spectral_centroid, spectral_energy, spectral_entropy, \n",
    "                            dominant_freq1, max_magnitude])\n",
    "        else:\n",
    "            features.extend([0, 0, 0, 0, 0])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def create_engineered_features(X_original):\n",
    "    \"\"\"\n",
    "    Convierte tensor (n, 128, 9) a (n, n_features_engineered)\n",
    "    \"\"\"\n",
    "    n_samples = X_original.shape[0]\n",
    "    all_features = []\n",
    "    \n",
    "    print(\"Extrayendo características de ingeniería de features...\")\n",
    "    for i in range(n_samples):\n",
    "        sequence = X_original[i]  # (128, 9)\n",
    "        \n",
    "        # Características temporales\n",
    "        time_features = extract_time_domain_features(sequence)\n",
    "        \n",
    "        # Características frecuenciales\n",
    "        freq_features = extract_frequency_domain_features(sequence)\n",
    "        \n",
    "        # Combinar todas las características\n",
    "        combined_features = np.concatenate([time_features, freq_features])\n",
    "        all_features.append(combined_features)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Procesadas {i}/{n_samples} muestras...\")\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "def improved_decision_tree(X, y,\n",
    "                          test_size=0.2,\n",
    "                          random_state=9,\n",
    "                          max_depth=15,  # Aumentado para datos más complejos\n",
    "                          min_samples_split=20,  # Aumentado para evitar overfitting\n",
    "                          min_samples_leaf=10,   # Aumentado para evitar overfitting\n",
    "                          max_features='sqrt',\n",
    "                          criterion='gini',\n",
    "                          verbose=True):\n",
    "    \"\"\"\n",
    "    Versión mejorada del Decision Tree para datos de sensores\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    def gini(y_subset):\n",
    "        if len(y_subset) == 0:\n",
    "            return 0.0\n",
    "        counts = np.array(list(Counter(y_subset).values()), dtype=float)\n",
    "        p = counts / counts.sum()\n",
    "        return 1.0 - np.sum(p**2)\n",
    "\n",
    "    def weighted_impurity(left, right):\n",
    "        nL, nR = len(left), len(right)\n",
    "        n = nL + nR\n",
    "        if n == 0: return 0.0\n",
    "        return (nL / n) * gini(left) + (nR / n) * gini(right)\n",
    "\n",
    "    def candidate_thresholds(col):\n",
    "        vals = np.unique(col)\n",
    "        if len(vals) <= 20:\n",
    "            return (vals[:-1] + vals[1:]) / 2.0\n",
    "        # Para muchas características, usar percentiles\n",
    "        pct = np.linspace(5, 95, num=10)  # 10 percentiles entre 5% y 95%\n",
    "        qs = np.percentile(col, pct)\n",
    "        return np.unique(qs)\n",
    "\n",
    "    def best_split(X_arr, y_arr):\n",
    "        best = {'feature_idx': None, 'threshold': None, 'imp': float('inf')}\n",
    "        n_features = X_arr.shape[1]\n",
    "\n",
    "        # Selección de features\n",
    "        if max_features == 'all':\n",
    "            feats = range(n_features)\n",
    "        elif max_features == 'sqrt':\n",
    "            k = max(1, int(np.sqrt(n_features)))\n",
    "            feats = rng.choice(range(n_features), size=k, replace=False)\n",
    "        elif isinstance(max_features, int):\n",
    "            k = min(max_features, n_features)\n",
    "            feats = rng.choice(range(n_features), size=k, replace=False)\n",
    "        else:\n",
    "            feats = range(n_features)\n",
    "\n",
    "        for fi in feats:\n",
    "            col = X_arr[:, fi]\n",
    "            if np.std(col) < 1e-8:  # Saltar características constantes\n",
    "                continue\n",
    "                \n",
    "            thr_candidates = candidate_thresholds(col)\n",
    "            for thr in thr_candidates:\n",
    "                left_mask = col <= thr\n",
    "                right_mask = col > thr\n",
    "                \n",
    "                if left_mask.sum() < min_samples_leaf or right_mask.sum() < min_samples_leaf:\n",
    "                    continue\n",
    "                    \n",
    "                imp = weighted_impurity(y_arr[left_mask], y_arr[right_mask])\n",
    "                if imp < best['imp']:\n",
    "                    best = {'feature_idx': fi, 'threshold': thr, 'imp': imp}\n",
    "        return best\n",
    "\n",
    "    def majority_class(y_subset):\n",
    "        return Counter(y_subset).most_common(1)[0][0]\n",
    "\n",
    "    def build_tree(X_arr, y_arr, depth=0):\n",
    "        n = len(y_arr)\n",
    "        classes = np.unique(y_arr)\n",
    "        \n",
    "        # Condiciones de parada más robustas\n",
    "        if (n < min_samples_split) or (depth >= max_depth) or (len(classes) == 1):\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "            \n",
    "        split = best_split(X_arr, y_arr)\n",
    "        if split['feature_idx'] is None:\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "\n",
    "        fi, thr = split['feature_idx'], split['threshold']\n",
    "        left_mask = X_arr[:, fi] <= thr\n",
    "        right_mask = X_arr[:, fi] > thr\n",
    "\n",
    "        if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
    "            return {'type': 'leaf', 'class': int(majority_class(y_arr)), 'n': int(n)}\n",
    "\n",
    "        left = build_tree(X_arr[left_mask], y_arr[left_mask], depth + 1)\n",
    "        right = build_tree(X_arr[right_mask], y_arr[right_mask], depth + 1)\n",
    "        \n",
    "        return {'type': 'node',\n",
    "                'feature_idx': int(fi),\n",
    "                'threshold': float(thr),\n",
    "                'left': left,\n",
    "                'right': right,\n",
    "                'n': int(n)}\n",
    "\n",
    "    def predict_single(node, x_row):\n",
    "        while node['type'] != 'leaf':\n",
    "            if x_row[node['feature_idx']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return int(node['class'])\n",
    "\n",
    "    def predict(tree, X_arr):\n",
    "        return np.array([predict_single(tree, row) for row in X_arr], dtype=int)\n",
    "\n",
    "    # ---- Preparar datos con feature engineering ----\n",
    "    print(\"Aplicando feature engineering...\")\n",
    "    X_engineered = create_engineered_features(X) if len(X.shape) == 3 else X\n",
    "    y_arr = np.array(y).astype(int)\n",
    "    \n",
    "    print(f\"Shape después de feature engineering: {X_engineered.shape}\")\n",
    "    \n",
    "    if isinstance(X_engineered, pd.DataFrame):\n",
    "        feature_names = X_engineered.columns.tolist()\n",
    "        X_arr = X_engineered.values\n",
    "    else:\n",
    "        X_arr = np.array(X_engineered)\n",
    "        feature_names = [f'feature_{i}' for i in range(X_arr.shape[1])]\n",
    "\n",
    "    # Split train/test\n",
    "    if test_size is None:\n",
    "        X_tr, y_tr = X_arr, y_arr\n",
    "        X_te, y_te = None, None\n",
    "    else:\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "            X_arr, y_arr, test_size=test_size,\n",
    "            stratify=y_arr,\n",
    "            random_state=random_state)\n",
    "\n",
    "    # Construir árbol\n",
    "    print(\"Entrenando árbol de decisión...\")\n",
    "    tree = build_tree(X_tr, y_tr)\n",
    "\n",
    "    # Métricas\n",
    "    if X_te is not None:\n",
    "        y_pred_test = predict(tree, X_te)\n",
    "        accuracy = accuracy_score(y_te, y_pred_test)\n",
    "        cm = confusion_matrix(y_te, y_pred_test)\n",
    "        f1 = f1_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "        prec = precision_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "        rec = recall_score(y_te, y_pred_test, average='weighted', zero_division=0)\n",
    "    else:\n",
    "        y_pred_test = None\n",
    "        accuracy = None; cm = None; f1 = None; prec = None; rec = None\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DECISION TREE MEJORADO - RESULTADOS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Dimensiones: {X_arr.shape}\")\n",
    "        print(f\"Hiperparámetros: max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
    "        print(f\"                min_samples_leaf={min_samples_leaf}, max_features={max_features}\")\n",
    "        \n",
    "        if cm is not None:\n",
    "            print(f\"\\nExactitud (Accuracy): {accuracy:.4f}\")\n",
    "            print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "            print(f\"Precision (weighted): {prec:.4f}\")\n",
    "            print(f\"Recall (weighted): {rec:.4f}\")\n",
    "            print(f\"\\nMatriz de Confusión:\\n{cm}\")\n",
    "        else:\n",
    "            print(\"No se realizó división train/test (test_size=None)\")\n",
    "\n",
    "    return {\n",
    "        'tree': tree,\n",
    "        'feature_names': feature_names,\n",
    "        'X_engineered': X_engineered,\n",
    "        'X_train': X_tr, 'X_test': X_te, 'y_train': y_tr, 'y_test': y_te,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'confusion_matrix': cm,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1, 'precision': prec, 'recall': rec\n",
    "    }\n",
    "\n",
    "def predict_from_tree(tree, X_arr):\n",
    "    \"\"\"Predict robusto para el árbol mejorado\"\"\"\n",
    "    def predict_one(node, x_row):\n",
    "        while node['type'] != 'leaf':\n",
    "            if x_row[node['feature_idx']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return int(node['class'])\n",
    "    \n",
    "    return np.array([predict_one(tree, row) for row in X_arr], dtype=int)\n",
    "\n",
    "# CÓDIGO PRINCIPAL MEJORADO\n",
    "print(\"=== PROCESAMIENTO DE DATOS HAR ===\")\n",
    "\n",
    "# 1. Preparar datos de entrenamiento con feature engineering\n",
    "print(\"Preparando datos de entrenamiento...\")\n",
    "X_train_engineered = create_engineered_features(X_train)\n",
    "y_train_processed = y_train\n",
    "\n",
    "print(f\"Train features shape: {X_train_engineered.shape}\")\n",
    "print(f\"Train labels shape: {y_train_processed.shape}\")\n",
    "\n",
    "# 2. Entrenar modelo mejorado\n",
    "print(\"\\nEntrenando modelo mejorado...\")\n",
    "res = improved_decision_tree(\n",
    "    X_train,  # El feature engineering se hace internamente\n",
    "    y_train_processed,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    max_depth=20,\n",
    "    min_samples_split=15,\n",
    "    min_samples_leaf=8,\n",
    "    max_features='sqrt',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 3. Preparar datos de test para predicción final\n",
    "print(\"\\nPreparando datos de test...\")\n",
    "X_test_engineered = create_engineered_features(X_testORI)\n",
    "print(f\"Test features shape: {X_test_engineered.shape}\")\n",
    "\n",
    "# 4. Predecir en test externo\n",
    "print(\"Realizando predicciones en test externo...\")\n",
    "y_pred_test_final = predict_from_tree(res['tree'], X_test_engineered)\n",
    "\n",
    "# 5. Guardar resultados para Kaggle\n",
    "print(\"Guardando resultados...\")\n",
    "results = pd.DataFrame({\n",
    "    'ID': np.arange(1, len(y_pred_test_final) + 1),\n",
    "    'Prediction': y_pred_test_final + 1  # Convertir 0-5 back to 1-6\n",
    "})\n",
    "\n",
    "results.to_csv(\"KaggleUpload_improved.csv\", index=False)\n",
    "print(f\"Predicciones guardadas en KaggleUpload_improved.csv\")\n",
    "print(f\"Shape del archivo de resultados: {results.shape}\")\n",
    "\n",
    "# 6. Mostrar distribución de predicciones\n",
    "print(\"\\nDistribución de predicciones:\")\n",
    "pred_counts = pd.Series(y_pred_test_final + 1).value_counts().sort_index()\n",
    "for pred, count in pred_counts.items():\n",
    "    activity_name = activity_labels[activity_labels['id'] == pred]['activity'].values[0]\n",
    "    print(f\"  Clase {pred} ({activity_name}): {count} muestras\")\n",
    "\n",
    "print(\"\\n¡Proceso completado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe92785",
   "metadata": {},
   "source": [
    "## Decision Tree con hiperparámetros sin optimizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f5730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (impl. propia) — métricas (test):\n",
      "max_depth: 8 min_samples_split: 10 min_samples_leaf: 4 max_features: sqrt\n",
      "Confusion matrix:\n",
      " [[139  55  40   0  11   0]\n",
      " [ 41 143  23   1   7   0]\n",
      " [ 45  34 110   4   4   0]\n",
      " [  3   0   1 229  24   0]\n",
      " [  6   0   2  29 238   0]\n",
      " [  0   0   0   2   0 280]]\n",
      "F1 (weighted): 0.7727  Precision: 0.7721  Recall: 0.7743\n",
      "=== Evaluación en tu TEST externo ===\n",
      "Confusion matrix:\n",
      " [[173  44  18   0  10   0]\n",
      " [ 30 156  18   0  11   0]\n",
      " [ 35  30 125   1   6   0]\n",
      " [  6   0   0 230  21   0]\n",
      " [  7   0   2  23 243   0]\n",
      " [  0   0   0   2   0 280]]\n",
      "F1: 0.8199  Precision: 0.8214  Recall: 0.8205\n"
     ]
    }
   ],
   "source": [
    "# tu partición propuesta:\n",
    "\n",
    "Xn = X_train.reshape(X_train.shape[0], -1)\n",
    "yn= y_train\n",
    "Xn_test  = X_testORI.reshape(X_testORI.shape[0], -1)\n",
    "\n",
    "\n",
    "# Entrenar árbol en X_train_final\n",
    "res = train_and_evaluate_decision_tree(\n",
    "    Xn, yn,\n",
    "    test_size=0.2,            # aquí si quieres que la función cree su propio test interno, o usa None para no dividir\n",
    "    random_state=20,\n",
    "    max_depth=8,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features='sqrt',      # suele ayudar a generalizar\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluar en tu test externo (X_test) con predict_from_tree\n",
    "y_pred_test_ext = predict_from_tree(res['tree'], X_test)\n",
    "cm_ext = confusion_matrix(y_test, y_pred_test_ext)\n",
    "f1_ext = f1_score(y_test, y_pred_test_ext, average='weighted')\n",
    "prec_ext = precision_score(y_test, y_pred_test_ext, average='weighted', zero_division=0)\n",
    "rec_ext = recall_score(y_test, y_pred_test_ext, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=== Evaluación en tu TEST externo ===\")\n",
    "print(\"Confusion matrix:\\n\", cm_ext)\n",
    "print(f\"F1: {f1_ext:.4f}  Precision: {prec_ext:.4f}  Recall: {rec_ext:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea1027",
   "metadata": {},
   "source": [
    "## Optimizador de hiperparámetros del Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d875bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from time import time\n",
    "\n",
    "def optimize_dt_hyperparams(X_train_final, y_train_final, X_val, y_val,\n",
    "                            param_grid = None,\n",
    "                            scoring = 'f1_weighted',\n",
    "                            random_state=42,\n",
    "                            verbose=True):\n",
    "    \"\"\"\n",
    "    Grid search sobre hiperparámetros para el Decision Tree implementado.\n",
    "    Entrena con X_train_final/y_train_final (sin split interno) y evalúa en X_val/y_val.\n",
    "    Devuelve dict con best_params, best_tree y DataFrame con resultados.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "\n",
    "    if param_grid is None:\n",
    "        # grilla por defecto razonable (no muy grande)\n",
    "        param_grid = {\n",
    "            'max_depth':        [6, 8, 10],\n",
    "            'min_samples_split':[2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features':     ['all', 'sqrt', 10],\n",
    "        }\n",
    "\n",
    "    # helper: predecir desde la estructura del árbol\n",
    "    def predict_from_tree_local(tree, X):\n",
    "        def one(node, x):\n",
    "            while node['type'] != 'leaf':\n",
    "                node = node['left'] if x[node['feature_idx']] <= node['threshold'] else node['right']\n",
    "            return int(node['class'])\n",
    "        return np.array([one(tree, np.asarray(x)) for x in X], dtype=int)\n",
    "\n",
    "    results = []\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    best_tree = None\n",
    "\n",
    "    combos = list(product(param_grid['max_depth'],\n",
    "                          param_grid['min_samples_split'],\n",
    "                          param_grid['min_samples_leaf'],\n",
    "                          param_grid['max_features']))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Grid search: {len(combos)} combinaciones. Empezando...\")\n",
    "\n",
    "    t0 = time()\n",
    "    for idx, (md, mss, msl, mf) in enumerate(combos, start=1):\n",
    "        t_iter = time()\n",
    "        # Entrenar en todo X_train_final (train_and_evaluate_decision_tree con test_size=None devuelve árbol)\n",
    "        res = train_and_evaluate_decision_tree(\n",
    "            X_train_final, y_train_final,\n",
    "            test_size=None,\n",
    "            verbose=False,\n",
    "            random_state=random_state,\n",
    "            max_depth=md,\n",
    "            min_samples_split=mss,\n",
    "            min_samples_leaf=msl,\n",
    "            max_features=mf\n",
    "        )\n",
    "        tree = res['tree']\n",
    "        val_pred = predict_from_tree_local(tree, np.array(X_val))\n",
    "        val_f1 = f1_score(y_val, val_pred, average='weighted', zero_division=0)\n",
    "\n",
    "        results.append({\n",
    "            'max_depth': md,\n",
    "            'min_samples_split': mss,\n",
    "            'min_samples_leaf': msl,\n",
    "            'max_features': mf,\n",
    "            'val_f1': val_f1\n",
    "        })\n",
    "\n",
    "        # actualizar mejor\n",
    "        if val_f1 > best_score + 1e-12:\n",
    "            best_score = val_f1\n",
    "            best_params = {'max_depth': md, 'min_samples_split': mss,\n",
    "                           'min_samples_leaf': msl, 'max_features': mf}\n",
    "            best_tree = tree\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[{idx}/{len(combos)}] md={md} mss={mss} msl={msl} mf={mf} -> val_f1={val_f1:.4f}  (iter time {time()-t_iter:.1f}s)\")\n",
    "\n",
    "    total_time = time() - t0\n",
    "    if verbose:\n",
    "        print(f\"Grid search terminado en {total_time:.1f}s. Mejor f1={best_score:.4f} con {best_params}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values('val_f1', ascending=False).reset_index(drop=True)\n",
    "    return {'best_params': best_params, 'best_tree': best_tree, 'results_df': results_df}\n",
    "\n",
    "\n",
    "# función predict_from_tree pública (si no la tienes en el scope)\n",
    "def predict_from_tree(tree, X_arr):\n",
    "    \"\"\"Predict robusto sobre un árbol construido por train_and_evaluate_decision_tree.\"\"\"\n",
    "    def predict_one(node, x_row):\n",
    "        while node['type'] != 'leaf':\n",
    "            if x_row[node['feature_idx']] <= node['threshold']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        return int(node['class'])\n",
    "    return np.array([predict_one(tree, row) for row in X_arr], dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b4400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search: 81 combinaciones. Empezando...\n",
      "[1/81] md=6 mss=2 msl=1 mf=all -> val_f1=0.7094  (iter time 69.1s)\n",
      "[2/81] md=6 mss=2 msl=1 mf=sqrt -> val_f1=0.7036  (iter time 2.4s)\n",
      "[3/81] md=6 mss=2 msl=1 mf=10 -> val_f1=0.7160  (iter time 1.1s)\n",
      "[4/81] md=6 mss=2 msl=2 mf=all -> val_f1=0.7094  (iter time 66.1s)\n",
      "[5/81] md=6 mss=2 msl=2 mf=sqrt -> val_f1=0.7028  (iter time 2.2s)\n",
      "[6/81] md=6 mss=2 msl=2 mf=10 -> val_f1=0.7051  (iter time 0.7s)\n",
      "[7/81] md=6 mss=2 msl=4 mf=all -> val_f1=0.7064  (iter time 74.1s)\n",
      "[8/81] md=6 mss=2 msl=4 mf=sqrt -> val_f1=0.7041  (iter time 2.0s)\n",
      "[9/81] md=6 mss=2 msl=4 mf=10 -> val_f1=0.7042  (iter time 0.7s)\n",
      "[10/81] md=6 mss=5 msl=1 mf=all -> val_f1=0.7094  (iter time 65.7s)\n",
      "[11/81] md=6 mss=5 msl=1 mf=sqrt -> val_f1=0.7061  (iter time 2.1s)\n",
      "[12/81] md=6 mss=5 msl=1 mf=10 -> val_f1=0.7035  (iter time 0.8s)\n",
      "[13/81] md=6 mss=5 msl=2 mf=all -> val_f1=0.7094  (iter time 68.4s)\n",
      "[14/81] md=6 mss=5 msl=2 mf=sqrt -> val_f1=0.7061  (iter time 1.9s)\n",
      "[15/81] md=6 mss=5 msl=2 mf=10 -> val_f1=0.7035  (iter time 0.7s)\n",
      "[16/81] md=6 mss=5 msl=4 mf=all -> val_f1=0.7064  (iter time 63.6s)\n",
      "[17/81] md=6 mss=5 msl=4 mf=sqrt -> val_f1=0.7041  (iter time 1.9s)\n",
      "[18/81] md=6 mss=5 msl=4 mf=10 -> val_f1=0.7051  (iter time 0.7s)\n",
      "[19/81] md=6 mss=10 msl=1 mf=all -> val_f1=0.7059  (iter time 62.8s)\n",
      "[20/81] md=6 mss=10 msl=1 mf=sqrt -> val_f1=0.7192  (iter time 2.0s)\n",
      "[21/81] md=6 mss=10 msl=1 mf=10 -> val_f1=0.6988  (iter time 0.6s)\n",
      "[22/81] md=6 mss=10 msl=2 mf=all -> val_f1=0.7059  (iter time 59.4s)\n",
      "[23/81] md=6 mss=10 msl=2 mf=sqrt -> val_f1=0.7192  (iter time 1.8s)\n",
      "[24/81] md=6 mss=10 msl=2 mf=10 -> val_f1=0.6988  (iter time 0.6s)\n",
      "[25/81] md=6 mss=10 msl=4 mf=all -> val_f1=0.7059  (iter time 59.4s)\n",
      "[26/81] md=6 mss=10 msl=4 mf=sqrt -> val_f1=0.7192  (iter time 2.0s)\n",
      "[27/81] md=6 mss=10 msl=4 mf=10 -> val_f1=0.6975  (iter time 0.8s)\n",
      "[28/81] md=8 mss=2 msl=1 mf=all -> val_f1=0.7535  (iter time 111.7s)\n",
      "[29/81] md=8 mss=2 msl=1 mf=sqrt -> val_f1=0.7526  (iter time 3.2s)\n",
      "[30/81] md=8 mss=2 msl=1 mf=10 -> val_f1=0.7596  (iter time 1.2s)\n",
      "[31/81] md=8 mss=2 msl=2 mf=all -> val_f1=0.7525  (iter time 110.4s)\n",
      "[32/81] md=8 mss=2 msl=2 mf=sqrt -> val_f1=0.7496  (iter time 3.2s)\n",
      "[33/81] md=8 mss=2 msl=2 mf=10 -> val_f1=0.7625  (iter time 1.5s)\n",
      "[34/81] md=8 mss=2 msl=4 mf=all -> val_f1=0.7573  (iter time 113.8s)\n",
      "[35/81] md=8 mss=2 msl=4 mf=sqrt -> val_f1=0.7493  (iter time 3.2s)\n",
      "[36/81] md=8 mss=2 msl=4 mf=10 -> val_f1=0.7490  (iter time 1.3s)\n",
      "[37/81] md=8 mss=5 msl=1 mf=all -> val_f1=0.7525  (iter time 115.0s)\n",
      "[38/81] md=8 mss=5 msl=1 mf=sqrt -> val_f1=0.7596  (iter time 3.2s)\n",
      "[39/81] md=8 mss=5 msl=1 mf=10 -> val_f1=0.7422  (iter time 1.1s)\n",
      "[40/81] md=8 mss=5 msl=2 mf=all -> val_f1=0.7525  (iter time 111.5s)\n",
      "[41/81] md=8 mss=5 msl=2 mf=sqrt -> val_f1=0.7604  (iter time 3.4s)\n",
      "[42/81] md=8 mss=5 msl=2 mf=10 -> val_f1=0.7411  (iter time 1.2s)\n",
      "[43/81] md=8 mss=5 msl=4 mf=all -> val_f1=0.7573  (iter time 112.1s)\n",
      "[44/81] md=8 mss=5 msl=4 mf=sqrt -> val_f1=0.7553  (iter time 3.0s)\n",
      "[45/81] md=8 mss=5 msl=4 mf=10 -> val_f1=0.7455  (iter time 1.0s)\n",
      "[46/81] md=8 mss=10 msl=1 mf=all -> val_f1=0.7507  (iter time 109.1s)\n",
      "[47/81] md=8 mss=10 msl=1 mf=sqrt -> val_f1=0.7583  (iter time 3.9s)\n",
      "[48/81] md=8 mss=10 msl=1 mf=10 -> val_f1=0.7389  (iter time 1.1s)\n",
      "[49/81] md=8 mss=10 msl=2 mf=all -> val_f1=0.7507  (iter time 110.6s)\n",
      "[50/81] md=8 mss=10 msl=2 mf=sqrt -> val_f1=0.7583  (iter time 4.5s)\n",
      "[51/81] md=8 mss=10 msl=2 mf=10 -> val_f1=0.7389  (iter time 1.6s)\n",
      "[52/81] md=8 mss=10 msl=4 mf=all -> val_f1=0.7572  (iter time 119.6s)\n",
      "[53/81] md=8 mss=10 msl=4 mf=sqrt -> val_f1=0.7573  (iter time 3.5s)\n",
      "[54/81] md=8 mss=10 msl=4 mf=10 -> val_f1=0.7371  (iter time 1.1s)\n",
      "[55/81] md=10 mss=2 msl=1 mf=all -> val_f1=0.7810  (iter time 176.1s)\n",
      "[56/81] md=10 mss=2 msl=1 mf=sqrt -> val_f1=0.7460  (iter time 5.0s)\n",
      "[57/81] md=10 mss=2 msl=1 mf=10 -> val_f1=0.7725  (iter time 1.9s)\n",
      "[58/81] md=10 mss=2 msl=2 mf=all -> val_f1=0.7811  (iter time 187.9s)\n",
      "[59/81] md=10 mss=2 msl=2 mf=sqrt -> val_f1=0.7750  (iter time 6.6s)\n",
      "[60/81] md=10 mss=2 msl=2 mf=10 -> val_f1=0.7702  (iter time 2.0s)\n",
      "[61/81] md=10 mss=2 msl=4 mf=all -> val_f1=0.7773  (iter time 185.2s)\n",
      "[62/81] md=10 mss=2 msl=4 mf=sqrt -> val_f1=0.7772  (iter time 5.3s)\n",
      "[63/81] md=10 mss=2 msl=4 mf=10 -> val_f1=0.7523  (iter time 1.6s)\n",
      "[64/81] md=10 mss=5 msl=1 mf=all -> val_f1=0.7793  (iter time 184.6s)\n",
      "[65/81] md=10 mss=5 msl=1 mf=sqrt -> val_f1=0.7680  (iter time 5.8s)\n",
      "[66/81] md=10 mss=5 msl=1 mf=10 -> val_f1=0.7624  (iter time 2.0s)\n",
      "[67/81] md=10 mss=5 msl=2 mf=all -> val_f1=0.7811  (iter time 171.3s)\n",
      "[68/81] md=10 mss=5 msl=2 mf=sqrt -> val_f1=0.7682  (iter time 6.8s)\n",
      "[69/81] md=10 mss=5 msl=2 mf=10 -> val_f1=0.7616  (iter time 2.6s)\n",
      "[70/81] md=10 mss=5 msl=4 mf=all -> val_f1=0.7773  (iter time 197.0s)\n",
      "[71/81] md=10 mss=5 msl=4 mf=sqrt -> val_f1=0.7742  (iter time 6.9s)\n",
      "[72/81] md=10 mss=5 msl=4 mf=10 -> val_f1=0.7744  (iter time 2.3s)\n",
      "[73/81] md=10 mss=10 msl=1 mf=all -> val_f1=0.7752  (iter time 188.0s)\n",
      "[74/81] md=10 mss=10 msl=1 mf=sqrt -> val_f1=0.7933  (iter time 5.1s)\n",
      "[75/81] md=10 mss=10 msl=1 mf=10 -> val_f1=0.7532  (iter time 1.8s)\n",
      "[76/81] md=10 mss=10 msl=2 mf=all -> val_f1=0.7762  (iter time 166.0s)\n",
      "[77/81] md=10 mss=10 msl=2 mf=sqrt -> val_f1=0.7924  (iter time 5.1s)\n",
      "[78/81] md=10 mss=10 msl=2 mf=10 -> val_f1=0.7524  (iter time 1.8s)\n",
      "[79/81] md=10 mss=10 msl=4 mf=all -> val_f1=0.7790  (iter time 160.4s)\n",
      "[80/81] md=10 mss=10 msl=4 mf=sqrt -> val_f1=0.7598  (iter time 5.3s)\n",
      "[81/81] md=10 mss=10 msl=4 mf=10 -> val_f1=0.7763  (iter time 2.0s)\n",
      "Grid search terminado en 3356.3s. Mejor f1=0.7933 con {'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt'}\n",
      "\n",
      "=== Evaluación en TEST externo ===\n",
      "Confusion matrix:\n",
      " [[164  35  35   4   7   0]\n",
      " [ 52 135  23   2   3   0]\n",
      " [ 65  30  96   1   5   0]\n",
      " [  1   0   2 231  22   1]\n",
      " [  4   1   0  28 242   0]\n",
      " [  0   0   0   0   0 282]]\n",
      "F1 (weighted): 0.7796  Precision: 0.7810  Recall: 0.7818\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# EJEMPLO DE USO (reemplaza variables si hace falta)\n",
    "# ------------------------------\n",
    "# Asumiendo que tienes:\n",
    "# X_train_final, y_train_final, X_val, y_val, X_test, y_test\n",
    "# y que train_and_evaluate_decision_tree está definida en el entorno.\n",
    "\n",
    "# Ejecutar búsqueda:\n",
    "opt = optimize_dt_hyperparams(X_train_final, y_train_final, X_val, y_val,\n",
    "                              param_grid=None,   # usa la grilla por defecto (pequeña)\n",
    "                              verbose=True)\n",
    "\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(opt['best_params'])\n",
    "\n",
    "# Evaluar en test externo\n",
    "best_tree = opt['best_tree']\n",
    "y_pred_test = predict_from_tree(best_tree, np.array(X_test))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "prec = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\n=== Evaluación en TEST externo ===\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(f\"F1 (weighted): {f1:.4f}  Precision: {prec:.4f}  Recall: {rec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
