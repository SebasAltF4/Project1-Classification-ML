{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe4d9c7",
   "metadata": {},
   "source": [
    "# Feature Engineering con PyTS (HAR)\n",
    "\n",
    "En este notebook se construirá conjuntos de *features* basados en transformaciones de series temporales usando **PyTS** sobre el dataset HAR (ventanas de 2.56 s, 128 pasos, 9 canales).  \n",
    "\n",
    "**Salidas:**  \n",
    "- **Resumen de Características** (`../reports/tables/pyts_feature_summary.csv`): Un archivo de metadatos que documenta la configuración y dimensiones de cada conjunto de características generado, asegurando la trazabilidad. \n",
    "- **Set de Features PAA** (`../artifacts/pyts_features_PAA.npz`): Un archivo comprimido con características basadas en Piecewise Aggregate Approximation, que capturan la forma global de las series de tiempo.\n",
    "\n",
    "- **Set de Features BoP** (`../artifacts/pyts_features_BOP.npz`): Un archivo con características simbólicas basadas en Bag-of-Patterns, diseñadas para modelar la distribución de patrones locales y repetitivos.\n",
    "\n",
    "- **Set de Features de Imagen** (`../artifacts/pyts_features_GAF_pooled.npz`): Un archivo opcional con características extraídas de la representación de las series como imágenes (Gramian Angular Fields), capturando su textura y dinámica 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383251f",
   "metadata": {},
   "source": [
    "## 1. Carga y Normalización de Datos\n",
    "\n",
    "En este primer paso, cargamos las series de tiempo del archivo `har_processed.npz`, obteniendo `X_train` con dimensiones de muestras, pasos de tiempo y canales, junto con las etiquetas `y_train`. Posteriormente, aplicamos **normalización Z-score** de forma independiente para cada uno de los 9 canales.\n",
    "\n",
    "Este preprocesamiento es fundamental por dos razones: primero, garantiza que las señales de todos los sensores (acelerómetros y giroscopios) se encuentren en una escala común con media 0 y desviación estándar 1, evitando que los canales con magnitudes mayores dominen artificialmente el análisis. Segundo, muchas técnicas de análisis de series de tiempo, incluyendo PAA y SAX que se utilizarán posteriormente, son sensibles a la escala de los datos, por lo que la normalización asegura su funcionamiento estable y efectivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d930e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7352, 128, 9)\n",
      "y_train shape: (7352,)\n",
      "Canales: ['body_acc_x' 'body_acc_y' 'body_acc_z' 'body_gyro_x' 'body_gyro_y'\n",
      " 'body_gyro_z' 'total_acc_x' 'total_acc_y' 'total_acc_z']\n",
      "Canal body_acc_x: mean=-0.001, std=0.195\n",
      "Canal body_acc_y: mean=-0.000, std=0.122\n",
      "Canal body_acc_z: mean=-0.000, std=0.107\n",
      "Canal body_gyro_x: mean=0.001, std=0.407\n",
      "Canal body_gyro_y: mean=-0.001, std=0.382\n",
      "Canal body_gyro_z: mean=0.000, std=0.256\n",
      "Canal total_acc_x: mean=0.805, std=0.414\n",
      "Canal total_acc_y: mean=0.029, std=0.391\n",
      "Canal total_acc_z: mean=0.086, std=0.358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Canal</th>\n",
       "      <th>Media (≈0)</th>\n",
       "      <th>STD (≈1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>body_acc_x</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>body_acc_y</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>body_acc_z</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>body_gyro_x</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>body_gyro_y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>body_gyro_z</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total_acc_x</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>total_acc_y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>total_acc_z</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Canal  Media (≈0)  STD (≈1)\n",
       "0   body_acc_x         0.0       1.0\n",
       "1   body_acc_y        -0.0       1.0\n",
       "2   body_acc_z        -0.0       1.0\n",
       "3  body_gyro_x         0.0       1.0\n",
       "4  body_gyro_y         0.0       1.0\n",
       "5  body_gyro_z         0.0       1.0\n",
       "6  total_acc_x         0.0       1.0\n",
       "7  total_acc_y         0.0       1.0\n",
       "8  total_acc_z        -0.0       1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================\n",
    "# 1. Carga de datos desde el caché\n",
    "# ===============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta del archivo procesado\n",
    "har_processed_path = \"../data/har_processed.npz\"\n",
    "\n",
    "# Cargar\n",
    "data = np.load(har_processed_path, allow_pickle=True)\n",
    "X_train = data[\"X_train\"]       # (n, 128, 9)\n",
    "y_train = data[\"y_train\"]       # (n,)\n",
    "channel_names = data[\"channel_names\"]\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"Canales:\", channel_names)\n",
    "\n",
    "# ===============================\n",
    "# 2. Normalización Z-score por canal\n",
    "# ===============================\n",
    "# Forma de X_train: (n_muestras, n_pasos, n_canales)\n",
    "n, T, C = X_train.shape\n",
    "\n",
    "# Inicializamos array normalizado\n",
    "X_norm = np.zeros_like(X_train, dtype=np.float32)\n",
    "\n",
    "# Normalización independiente por canal\n",
    "for c in range(C):\n",
    "    mean_c = X_train[:, :, c].mean()\n",
    "    std_c  = X_train[:, :, c].std()\n",
    "    X_norm[:, :, c] = (X_train[:, :, c] - mean_c) / std_c\n",
    "    print(f\"Canal {channel_names[c]}: mean={mean_c:.3f}, std={std_c:.3f}\")\n",
    "\n",
    "# Reemplazamos X_train por versión normalizada\n",
    "X_train = X_norm\n",
    "\n",
    "# ===============================\n",
    "# 3. Verificación de normalización\n",
    "# ===============================\n",
    "# Calculamos media y std por canal en el dataset normalizado\n",
    "stats = []\n",
    "for c in range(C):\n",
    "    mean_c = X_train[:, :, c].mean()\n",
    "    std_c  = X_train[:, :, c].std()\n",
    "    stats.append([channel_names[c], round(mean_c, 3), round(std_c, 3)])\n",
    "\n",
    "df_stats = pd.DataFrame(stats, columns=[\"Canal\", \"Media (≈0)\", \"STD (≈1)\"])\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb346e9",
   "metadata": {},
   "source": [
    "## 2. Diseño de Estrategias de Características con `pyts`\n",
    "\n",
    "Para representar las series de tiempo, se diseñaron tres estrategias complementarias que capturan diferentes aspectos de las señales de movimiento:\n",
    "\n",
    "**Estrategia 1: Resumen Global con PAA (Piecewise Aggregate Approximation)**\n",
    "\n",
    "PAA divide la secuencia de 128 puntos en segmentos más pequeños y calcula el promedio de cada uno. Esto captura la forma general de la señal: actividades dinámicas como caminar muestran patrones ondulatorios, mientras que actividades estáticas como sentarse son casi planas. Se genera un vector concatenado de características para los 9 canales.\n",
    "\n",
    "**Estrategia 2: Patrones Locales con Bag-of-Patterns (BoP)**\n",
    "\n",
    "BoP discretiza la señal en símbolos mediante SAX y cuenta la frecuencia de patrones recurrentes. Esto permite distinguir micro-patrones entre actividades similares: aunque caminar y subir escaleras son periódicas, la forma exacta de cada paso difiere. Se genera un histograma de frecuencias por canal que se concatena.\n",
    "\n",
    "**Estrategia 3: Dinámica Visual con Imágenes (GAF o RP)**\n",
    "\n",
    "Esta técnica convierte las series temporales en imágenes bidimensionales que revelan patrones de autocorrelación y dinámica no lineal. Se aplica solo a los canales más informativos (acelerómetros del cuerpo) y se resume mediante pooling para obtener vectores compactos que capturan la textura temporal de la señal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_PAA: shape=(7352, 288), dtype=float64\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PyTS: F1 PAA, F2 BoP (SAX), F3 GAF + pooling (versión robusta)\n",
    "# ===============================\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "import inspect\n",
    "\n",
    "# -------- utilidades --------\n",
    "def _supports_param(cls, param_name):\n",
    "    return param_name in inspect.signature(cls.__init__).parameters\n",
    "\n",
    "def check_matrix(X, name):\n",
    "    assert isinstance(X, np.ndarray), f\"{name} no es np.ndarray\"\n",
    "    assert np.isfinite(X).all(), f\"{name} contiene NaN/Inf\"\n",
    "    print(f\"{name}: shape={X.shape}, dtype={X.dtype}\")\n",
    "\n",
    "# ===============================\n",
    "# F1. PAA (Piecewise Aggregate Approximation)\n",
    "# ===============================\n",
    "def features_paa(X, n_segments=32, copy=True, pad_mode='edge'):\n",
    "    \"\"\"\n",
    "    Devuelve (n, C * n_segments). Soporta PAA moderno (n_segments) o antiguo (window_size).\n",
    "    Si la API antigua requiere divisibilidad, hace padding automático.\n",
    "    \"\"\"\n",
    "    from pyts.approximation import PiecewiseAggregateApproximation as _PAA\n",
    "    if copy:\n",
    "        X = np.array(X, copy=True)\n",
    "    n, T, C = X.shape\n",
    "\n",
    "    use_n_segments = _supports_param(_PAA, 'n_segments')\n",
    "    if use_n_segments:\n",
    "        paa = _PAA(n_segments=n_segments)  # API moderna remapea a n_segments\n",
    "    else:\n",
    "        # API antigua: necesita window_size entero -> rellenamos si no divide\n",
    "        window_size = T // n_segments + (1 if T % n_segments else 0)\n",
    "        T_needed = window_size * n_segments\n",
    "        if T_needed != T:\n",
    "            X = np.pad(X, ((0,0), (0, T_needed - T), (0,0)), mode=pad_mode)\n",
    "        paa = _PAA(window_size=window_size)\n",
    "\n",
    "    feats = [paa.transform(X[:, :, c]) for c in range(C)]  # cada (n, n_segments)\n",
    "    X_paa = np.concatenate(feats, axis=1)                  # (n, C * n_segments)\n",
    "    return X_paa\n",
    "\n",
    "# ===============================\n",
    "# F2. Bag-of-Patterns/Words (SAX + ventana deslizante)\n",
    "# ===============================\n",
    "def features_bop(\n",
    "    X,\n",
    "    window_size=16,\n",
    "    word_size=8,\n",
    "    n_bins=6,\n",
    "    numerosity_reduction=True,\n",
    "    sparse=True,\n",
    "    strategy='normal'\n",
    "):\n",
    "    \"\"\"\n",
    "    Devuelve (n, sum n_words_c) denso. Hace fallback a BagOfWords (nuevas versiones)\n",
    "    y filtra kwargs según soporte real de la clase.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pyts.transformation import BagOfPatterns as _BoX\n",
    "    except ImportError:\n",
    "        from pyts.transformation import BagOfWords as _BoX  # versiones nuevas\n",
    "\n",
    "    def supports(k): \n",
    "        return k in inspect.signature(_BoX.__init__).parameters\n",
    "\n",
    "    kwargs_all = dict(\n",
    "        window_size=window_size,\n",
    "        word_size=word_size,\n",
    "        n_bins=n_bins,\n",
    "        numerosity_reduction=numerosity_reduction,\n",
    "        sparse=sparse,\n",
    "        strategy=strategy\n",
    "    )\n",
    "    kwargs = {k: v for k, v in kwargs_all.items() if supports(k)}\n",
    "\n",
    "    n, T, C = X.shape\n",
    "    bop = _BoX(**kwargs)\n",
    "    feats = []\n",
    "    for c in range(C):\n",
    "        H = bop.fit_transform(X[:, :, c])    # (n, n_words_c)\n",
    "        if issparse(H):\n",
    "            H = H.toarray()\n",
    "        feats.append(H)\n",
    "    X_bop = np.concatenate(feats, axis=1)\n",
    "    return X_bop\n",
    "\n",
    "# ===============================\n",
    "# F3. GAF (Gramian Angular Field) + pooling (opcional)\n",
    "# ===============================\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "def block_pool_2d(images, pool_h=8, pool_w=8):\n",
    "    \"\"\"\n",
    "    images: (n_samples, H, W)\n",
    "    Average pooling no solapado en bloques (pool_h x pool_w).\n",
    "    Devuelve: (n_samples, H/pool_h, W/pool_w)\n",
    "    \"\"\"\n",
    "    n, H, W = images.shape\n",
    "    assert H % pool_h == 0 and W % pool_w == 0, \\\n",
    "        f\"La imagen ({H}x{W}) debe ser múltiplo del pooling ({pool_h}x{pool_w}).\"\n",
    "    h_blocks = H // pool_h\n",
    "    w_blocks = W // pool_w\n",
    "    pooled = images.reshape(n, h_blocks, pool_h, w_blocks, pool_w).mean(axis=(2, 4))\n",
    "    return pooled\n",
    "\n",
    "def features_gaf_pooled(\n",
    "    X,\n",
    "    channel_indices,\n",
    "    image_size=64,\n",
    "    method='summation',  # 'summation' | 'difference'\n",
    "    pool_h=8, pool_w=8\n",
    "):\n",
    "    \"\"\"\n",
    "    X: (n, T, C) Z-normalizado\n",
    "    channel_indices: lista de canales a transformar\n",
    "    Devuelve: X_IMG (n, len(channel_indices) * (image_size/pool_h) * (image_size/pool_w))\n",
    "    \"\"\"\n",
    "    n, T, C = X.shape\n",
    "    # Sanitizar índices\n",
    "    ch = [int(ci) for ci in channel_indices if 0 <= int(ci) < C]\n",
    "    if len(ch) == 0:\n",
    "        raise ValueError(f\"channel_indices fuera de rango para C={C}\")\n",
    "    if image_size % pool_h != 0 or image_size % pool_w != 0:\n",
    "        raise ValueError(\"image_size debe ser múltiplo de pool_h y pool_w\")\n",
    "\n",
    "    gaf = GramianAngularField(image_size=image_size, method=method)\n",
    "    pooled_feats = []\n",
    "    for c in ch:\n",
    "        imgs = gaf.fit_transform(X[:, :, c])               # (n, H, W) con H=W=image_size\n",
    "        imgs_pooled = block_pool_2d(imgs, pool_h, pool_w)  # (n, H', W')\n",
    "        pooled_feats.append(imgs_pooled.reshape(n, -1))    # (n, H'*W')\n",
    "    X_img = np.concatenate(pooled_feats, axis=1)\n",
    "    return X_img\n",
    "\n",
    "# ===============================\n",
    "# Ejecutar (asumiendo X_train ya normalizado, shape (n, T, C))\n",
    "# ===============================\n",
    "# 1) PAA\n",
    "X_PAA = features_paa(X_train, n_segments=32)\n",
    "check_matrix(X_PAA, \"X_PAA\")\n",
    "\n",
    "# 2) BoP (SAX)\n",
    "X_BOP = features_bop(\n",
    "    X_train,\n",
    "    window_size=16,\n",
    "    word_size=8,\n",
    "    n_bins=6,\n",
    "    numerosity_reduction=True,\n",
    "    sparse=True,        # si no está soportado en tu versión, se ignora\n",
    "    strategy='normal'   # idem\n",
    ")\n",
    "check_matrix(X_BOP, \"X_BOP\")\n",
    "\n",
    "# 3) GAF + pooling\n",
    "# Intento de mapeo por nombre si existe channel_names; si no, fallback [0,1,2]\n",
    "try:\n",
    "    channel_names = [str(x) for x in channel_names]\n",
    "except NameError:\n",
    "    channel_names = None\n",
    "\n",
    "def _find_indices(names, wanted):\n",
    "    idx = []\n",
    "    for w in wanted:\n",
    "        if w in names:\n",
    "            idx.append(names.index(w))\n",
    "    return idx\n",
    "\n",
    "if channel_names:\n",
    "    wanted = [\"body_acc_x\",\"body_acc_y\",\"body_acc_z\"]\n",
    "    body_acc_idx = _find_indices(channel_names, wanted)\n",
    "    if len(body_acc_idx) == 0:\n",
    "        body_acc_idx = [0, 1, 2]  # fallback seguro\n",
    "else:\n",
    "    body_acc_idx = [0, 1, 2]\n",
    "\n",
    "X_IMG = features_gaf_pooled(\n",
    "    X_train,\n",
    "    channel_indices=body_acc_idx,\n",
    "    image_size=64,\n",
    "    method='summation',\n",
    "    pool_h=8, pool_w=8\n",
    ")\n",
    "check_matrix(X_IMG, \"X_IMG (GAF pooled, body_acc)\")\n",
    "\n",
    "# Resumen\n",
    "summary = [\n",
    "    (\"PAA\", X_PAA.shape[1]),\n",
    "    (\"BoP\", X_BOP.shape[1]),\n",
    "    (\"GAF_pooled(body_acc)\", X_IMG.shape[1]),\n",
    "]\n",
    "for name, d in summary:\n",
    "    print(f\"{name:22s} -> n_features = {d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41cde54",
   "metadata": {},
   "source": [
    "## 3. Selección y Justificación de Hiperparámetros\n",
    "\n",
    "La efectividad de las transformaciones de `pyts` depende de sus hiperparámetros. Los valores seleccionados equilibran capacidad representativa y simplicidad del modelo.\n",
    "\n",
    "**Para PAA:**\n",
    "\n",
    "Se utiliza `n_segments = 32`, reduciendo cada señal de 128 a 32 puntos mediante promediado. Esto suaviza el ruido y preserva la forma estructural, generando 288 características totales (9 canales × 32).\n",
    "\n",
    "**Para Bag-of-Patterns:**\n",
    "\n",
    "Se configuran `window_size = 16` para capturar micro-movimientos de aproximadamente 0.32 segundos, `word_size = 8` para equilibrar descripción y complejidad en la representación SAX, y `n_bins = 6` para discretizar la señal con granularidad adecuada sin excesiva sensibilidad al ruido.\n",
    "\n",
    "**Para Imágenes (GAF o RP):**\n",
    "\n",
    "Se genera `image_size = 64` con pooling a `8x8`, creando una huella digital de 64 características por canal. Se aplica solo a los acelerómetros corporales `body_acc_x`, `body_acc_y` y `body_acc_z` por contener la mayor información discriminativa, generando 192 características totales (3 canales × 64)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ce314",
   "metadata": {},
   "source": [
    "## 4. Pipeline de construcción\n",
    "\n",
    "1. **Normalizar** `X_train` por canal (z-score).  \n",
    "2. **F1-PAA:** aplicar por canal → concatenar → `X_PAA` `(n, 9*M)`.  \n",
    "3. **F2-Bag-of-Patterns:** aplicar por canal → concatenar histogramas → `X_BOP`.  \n",
    "4. **F3-Imagen (opcional):** GAF/RP en 3 canales `body_acc_*` → *pooling* 8×8 → concatenar → `X_IMG`.  \n",
    "5. **Chequear varianza** y remover columnas constantes.  \n",
    "6. **Guardar artefactos** (`.npz` + CSV de resumen con dimensiones, tiempos y parámetros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f3a338",
   "metadata": {},
   "source": [
    "## 5. Validaciones rápidas (sin entrenamiento)\n",
    "\n",
    "- **Sanity checks:** dimensiones de salida, ausencia de NaN/inf, varianza > 0.  \n",
    "- **Separabilidad preliminar:** PCA/UMAP con `X_PAA` y `X_BOP` (2D) para visualizar si hay agrupamientos por actividad.  \n",
    "- **Correlaciones:** matriz de correlación entre *features* de F1 y F2 para descartar redundancias extremas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b1ec4",
   "metadata": {},
   "source": [
    "## 6. Exportación para el paper y trazabilidad\n",
    "\n",
    "- Guardar **resumen de *features*** (`../reports/tables/pyts_feature_summary.csv`) con:  \n",
    "  - `set_name`, `n_features`, `n_segments / window_size / word_size / n_bins`, canales usados, normalización y compute_time_sec (tiempo de cómputo en segundos que tomó generar cada conjunto de features).\n",
    "- Guardar **figuras** de:  \n",
    "  - PCA de `X_PAA` y de `X_BOP` (coloreado por actividad).  \n",
    "- Guardar artefactos:  \n",
    "  - `../artifacts/pyts_features_PAA.npz` (X_PAA, y, meta)  \n",
    "  - `../artifacts/pyts_features_BOP.npz` (X_BOP, y, meta)  \n",
    "  - `../artifacts/pyts_features_GAF_pooled.npz`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef44753",
   "metadata": {},
   "source": [
    "## 7. Limitaciones y plan para tsfresh\n",
    "\n",
    "- **PyTS** resume forma global (PAA) y patrones locales discretizados (BoP), pero no agota todos los estadísticos temporales.  \n",
    "- **tsfresh** complementará con **features estadísticas masivas** (autocorrelación, entropías, percentiles, coeficientes de Fourier), seguidas de **selección de *features***.  \n",
    "- En el notebook 03 (modelado), compararemos desempeño usando:  \n",
    "  - Solo PyTS (F1/F2)  \n",
    "  - Solo tsfresh  \n",
    "  - **Combinación PyTS + tsfresh** (si la dimensionalidad y el *overfitting* lo permiten)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e719476",
   "metadata": {},
   "source": [
    "## 8. Conclusiones (PyTS)\n",
    "\n",
    "- Se definieron y justificaron **tres familias** de *features* en PyTS: **PAA (compacto)**, **BoP (motivos locales)** y **GAF/RP (opcional, firma visual)**.  \n",
    "- Los parámetros propuestos priorizan **robustez, baja dimensionalidad e interpretabilidad**.  \n",
    "- Dejaremos listas las matrices de *features* y el resumen para integrarlas en el **notebook de modelado** y en el **paper** (sección Metodología/Experimentación).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
